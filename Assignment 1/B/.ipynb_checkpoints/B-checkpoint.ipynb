{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15808e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb9c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, y, k, random_seed=0):\n",
    "    \"\"\"\n",
    "    Implementation of K-Fold Cross Validation\n",
    "\n",
    "    Args:\n",
    "        X: Features\n",
    "        y: Targets\n",
    "        k: Number of folds\n",
    "        random_seed: A seed to make the randomization deterministic and reproducible\n",
    "\n",
    "    Returns:\n",
    "        A list containing k tuples of the form (X_train, y_train, X_val, y_val) corresponding to the k folds\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(X)\n",
    "    np.random.shuffle(y)\n",
    "    X_folds = np.array_split(X, k)\n",
    "    y_folds = np.array_split(y, k)\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        x_train = np.concatenate(X_folds[:i] + X_folds[i+1:])\n",
    "        y_train = np.concatenate(y_folds[:i] + y_folds[i+1:])\n",
    "        x_val = X_folds[i]\n",
    "        y_val = y_folds[i]\n",
    "        folds.append((x_train, y_train, x_val, y_val))\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "ae12c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeLoss(y_target, y_pred, params, bias, loss_fn, regularization, regularization_scaling_factor, average=True):\n",
    "    \"\"\"\n",
    "    Wrapper function which wraps different loss functions (Only L2 Loss Implemented Here as Linear Regression used Least Squares Optimization)\n",
    "\n",
    "    Args:\n",
    "        y_target: Target Values\n",
    "        y_pred: Predicted Values\n",
    "        params: Parameters of the Model (Needed for Regularization)\n",
    "        bias: Bias of the Model (Needed for Regularization)\n",
    "        loss_fn: Loss Function to use (L2)\n",
    "        regularization: Regularization to use (No, Lasso, Ridge)\n",
    "        regularization_scaling_factor: Scaling Factor for Regularization\n",
    "        average: A bool value indicating whether to average the loss or not\n",
    "\n",
    "    Returns:\n",
    "        A float value containing the loss\n",
    "    \"\"\"\n",
    "    if loss_fn == \"L2\":\n",
    "        return L2_loss(y_target, y_pred, regularization, params, bias, regularization_scaling_factor, average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "991c14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_loss(y_target, y_pred, regularization, params, bias, regularization_scaling_factor, average=True):\n",
    "    \"\"\"\n",
    "    Implementation of L2 Loss\n",
    "\n",
    "    Args:\n",
    "        y_target: Target Values\n",
    "        y_pred: Predicted Values\n",
    "        regularization: Regularization to use ('No', Lasso, Ridge)\n",
    "        params: Parameters of the Model (Needed for Regularization)\n",
    "        bias: Bias of the Model (Needed for Regularization)\n",
    "        regularization_scaling_factor: Scaling Factor for Regularization\n",
    "        average: A bool value indicating whether to average the loss or not\n",
    "\n",
    "    Returns:\n",
    "        A float value containing the total loss (Not Mean Loss)\n",
    "    \"\"\"\n",
    "    if regularization == 'Lasso':\n",
    "        Lasso_loss = np.sum(np.square(y_target - y_pred)) + regularization_scaling_factor*(np.sum(np.abs(params)))\n",
    "        if average:\n",
    "            return Lasso_loss / len(y_target)\n",
    "        return Lasso_loss\n",
    "    elif regularization == 'Ridge':\n",
    "        Ridge_loss = np.sum(np.square(y_target - y_pred)) + regularization_scaling_factor*(np.sum(np.square(params)))\n",
    "        if average:\n",
    "            return Ridge_loss / len(y_target)\n",
    "        return Ridge_loss\n",
    "    else:\n",
    "        loss = np.sum(np.square(y_target - y_pred))\n",
    "        if average:\n",
    "            return loss / len(y_target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "b1541a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_target, y_pred):\n",
    "    \"\"\"\n",
    "    Implementation of Root Mean Squared Error\n",
    "\n",
    "    Args: \n",
    "        y_target: Target Values\n",
    "        y_pred: Predicted Values\n",
    "    \n",
    "    Returns:\n",
    "        A float value containing the RMSE\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean(np.square(y_target - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "e76334c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_RMSE(ls_RMSE, split, fold, show, total_folds, learning_rate, regularization, regularization_scaling_factor):\n",
    "    \"\"\"\n",
    "    Implementation of a function to plot and save the RMSE trends over the epochs and optionally display it\n",
    "\n",
    "    Args:\n",
    "        ls_RMSE: A list containing the RMSE values over the epochs\n",
    "        split: The Split Name\n",
    "        fold: The fold number\n",
    "        show: A bool value indicating whether to show the plot or not\n",
    "        total_folds: Total number of folds\n",
    "        learning_rate: Learning Rate used for training\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    plt.plot(ls_RMSE)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(f\"RMSE vs Epochs for {split} Fold {fold} of Total Folds {total_folds} (Learning Rate = {learning_rate}, Regularization = {regularization}, Regularization Scaling Factor = {'NA' if regularization == 'No' else regularization_scaling_factor})\", loc='center', wrap=True)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.savefig(f\"plots/split_{split}_RMSE_lr={lr}_totalFolds_{total_folds}_fold_{fold}_reg={regularization}_regScalingFactor={regularization_scaling_factor}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "7cbf324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_with_gradient_descent(x_train, \n",
    "                                            y_train, \n",
    "                                            x_val, \n",
    "                                            y_val, \n",
    "                                            epochs, \n",
    "                                            lr, \n",
    "                                            normalize='Min-Max',\n",
    "                                            regularization='No', \n",
    "                                            regularization_scaling_factor=1.0,\n",
    "                                            useEarlyStopping=False, \n",
    "                                            EarlyStopping_patience=10,\n",
    "                                            EarlyStopping_min_delta=0.001,\n",
    "                                            random_seed=0, \n",
    "                                            verbose=1):\n",
    "    \"\"\"\n",
    "    Implementation of Linear Regression with Gradient Descent\n",
    "\n",
    "    Args:\n",
    "        x_train: Training Features\n",
    "        y_train: Training Targets\n",
    "        x_val: Validation Features\n",
    "        y_val: Validation Targets\n",
    "        epochs: Number of epochs to train the model\n",
    "        lr: Learning Rate\n",
    "        regularization: Regularization to use (No, Lasso, Ridge)\n",
    "        regularization_scaling_factor: Scaling Factor for Regularization\n",
    "        useEarlyStopping: A bool value indicating whether to use Early Stopping or not\n",
    "        EarlyStopping_patience: Number of epochs to wait before stopping if the validation loss does not decrease\n",
    "        EarlyStopping_min_delta: Minimum change in validation loss to qualify as an improvement\n",
    "        random_seed: A seed to make the randomization deterministic and reproducible\n",
    "        verbose: A flag to control the verbosity of the training process\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing the parameters and bias of the final model\n",
    "    \"\"\"\n",
    "    x_train, normalizer_min, normalizer_max = min_max_normalization(x_train)\n",
    "    x_val = (x_val - normalizer_min) / (normalizer_max - normalizer_min)\n",
    "    x_train = np.append([[1]]*len(x_train), x_train, axis=1)\n",
    "    x_val = np.append([[1]]*len(x_val), x_val, axis=1)\n",
    "    num_params = len(x_train[0])\n",
    "    params = np.random.RandomState(random_seed).normal(size=num_params)\n",
    "    \n",
    "    print(x_train.shape)\n",
    "    print(params.shape)\n",
    "    print(x_val.shape)\n",
    "    \n",
    "    # Derivatives of y with respect to params and bias\n",
    "    # y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + ... + beta_n * x_n\n",
    "    # y = bias + params[0] * x_1 + params[1] * x_2 + ... + params[n] * x_n\n",
    "    # y = bias + np.dot(params, x)\n",
    "    # dy/dbias = 1\n",
    "    # dy/dparams[0] = x_1\n",
    "    # dy/dparams[1] = x_2\n",
    "    # ...\n",
    "    # dy/dparams[n] = x_n\n",
    "\n",
    "    ls_loss = []\n",
    "    ls_RMSE_train = []\n",
    "    ls_RMSE_val = [] \n",
    "    earlyStopping = False\n",
    "    epoch = 0\n",
    "    if verbose == 2 or verbose == 3:\n",
    "        open('log.txt', 'w').close()\n",
    "    while epoch < epochs and not earlyStopping:\n",
    "        y_pred = np.dot(x_train, params)\n",
    "        loss = ComputeLoss(y_train, y_pred, params, bias, \"L2\", regularization, regularization_scaling_factor)\n",
    "        params[i] -= lr*np.sum(x_train*(y_pred-y_train))/len(x_train)\n",
    "#         for i in range(num_params):\n",
    "#             if regularization == 'No':\n",
    "#                 params[i] -= lr * np.sum(x_train[:, i] * (y_pred - y_train))/len(x_train)\n",
    "#             elif regularization == 'Lasso':\n",
    "#                 # print(lr * (np.sum(x_train[:, i] * (y_pred - y_train)) + regularization_scaling_factor*np.sign(params[i]))/len(x_train))\n",
    "#                 params[i] -= lr * (np.sum(x_train[:, i] * (y_pred - y_train))/len(x_train) + regularization_scaling_factor*np.sign(params[i]))\n",
    "#             elif regularization == 'Ridge':\n",
    "#                 params[i] -= lr * (np.sum(x_train[:, i] * (y_pred - y_train))/len(x_train) + regularization_scaling_factor*2*params[i]) \n",
    "        y_pred_val = np.dot(x_val, params)\n",
    "        loss_val = ComputeLoss(y_val, y_pred_val, params, bias, \"L2\", regularization, regularization_scaling_factor)\n",
    "        ls_loss.append(loss_val)\n",
    "        if useEarlyStopping:\n",
    "            earlyStopping = EarlyStopping(ls_loss, patience=EarlyStopping_patience, min_delta=EarlyStopping_min_delta)\n",
    "        RMSE_train = RMSE(y_train, y_pred)\n",
    "        RMSE_val = RMSE(y_val, y_pred_val)\n",
    "        ls_RMSE_train.append(RMSE_train)\n",
    "        ls_RMSE_val.append(RMSE_val)\n",
    "        message = f\"Epoch {epoch}: loss_train={loss:.3f}, loss_val={loss_val:.3f}, RMSE_train={RMSE_train:.3f}, RMSE_val={RMSE_val:.3f}\"\n",
    "        if verbose == 1:\n",
    "#             pass\n",
    "            print(message)\n",
    "        elif verbose == 2:\n",
    "            log_message(message, \"log.txt\")\n",
    "        elif verbose == 3:\n",
    "#             print(message)\n",
    "            log_message(message, \"log.txt\")\n",
    "        else:\n",
    "            pass\n",
    "        epoch += 1\n",
    "\n",
    "    return params, ls_RMSE_train, ls_RMSE_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "3662572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Real estate.csv')\n",
    "# df = df.sample(frac=1).reset_index(drop=True)\n",
    "# Since the dataset was released 4 years ago I assume the dates are relative to that and hence break the transaction date into years since last sold\n",
    "df['Year last sold'] = df['X1 transaction date'].apply(lambda x: int(str(x).split('.')[0]))\n",
    "df['Years since last sold'] = 2018 - df['Year last sold']\n",
    "X = df.drop(columns=['Y house price of unit area', 'X1 transaction date', 'No', 'Year last sold', 'Years since last sold']).to_numpy()\n",
    "y = df['Y house price of unit area'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "ab716297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X2 house age</th>\n",
       "      <th>X3 distance to the nearest MRT station</th>\n",
       "      <th>X4 number of convenience stores</th>\n",
       "      <th>X5 latitude</th>\n",
       "      <th>X6 longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>84.87882</td>\n",
       "      <td>10</td>\n",
       "      <td>24.98298</td>\n",
       "      <td>121.54024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.5</td>\n",
       "      <td>306.59470</td>\n",
       "      <td>9</td>\n",
       "      <td>24.98034</td>\n",
       "      <td>121.53951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.3</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.3</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>390.56840</td>\n",
       "      <td>5</td>\n",
       "      <td>24.97937</td>\n",
       "      <td>121.54245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>13.7</td>\n",
       "      <td>4082.01500</td>\n",
       "      <td>0</td>\n",
       "      <td>24.94155</td>\n",
       "      <td>121.50381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>5.6</td>\n",
       "      <td>90.45606</td>\n",
       "      <td>9</td>\n",
       "      <td>24.97433</td>\n",
       "      <td>121.54310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>18.8</td>\n",
       "      <td>390.96960</td>\n",
       "      <td>7</td>\n",
       "      <td>24.97923</td>\n",
       "      <td>121.53986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>8.1</td>\n",
       "      <td>104.81010</td>\n",
       "      <td>5</td>\n",
       "      <td>24.96674</td>\n",
       "      <td>121.54067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>6.5</td>\n",
       "      <td>90.45606</td>\n",
       "      <td>9</td>\n",
       "      <td>24.97433</td>\n",
       "      <td>121.54310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     X2 house age  X3 distance to the nearest MRT station  \\\n",
       "0            32.0                                84.87882   \n",
       "1            19.5                               306.59470   \n",
       "2            13.3                               561.98450   \n",
       "3            13.3                               561.98450   \n",
       "4             5.0                               390.56840   \n",
       "..            ...                                     ...   \n",
       "409          13.7                              4082.01500   \n",
       "410           5.6                                90.45606   \n",
       "411          18.8                               390.96960   \n",
       "412           8.1                               104.81010   \n",
       "413           6.5                                90.45606   \n",
       "\n",
       "     X4 number of convenience stores  X5 latitude  X6 longitude  \n",
       "0                                 10     24.98298     121.54024  \n",
       "1                                  9     24.98034     121.53951  \n",
       "2                                  5     24.98746     121.54391  \n",
       "3                                  5     24.98746     121.54391  \n",
       "4                                  5     24.97937     121.54245  \n",
       "..                               ...          ...           ...  \n",
       "409                                0     24.94155     121.50381  \n",
       "410                                9     24.97433     121.54310  \n",
       "411                                7     24.97923     121.53986  \n",
       "412                                5     24.96674     121.54067  \n",
       "413                                9     24.97433     121.54310  \n",
       "\n",
       "[414 rows x 5 columns]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['Y house price of unit area', 'X1 transaction date', 'No', 'Year last sold', 'Years since last sold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "63a6a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaler(data):\n",
    "    \"\"\"\n",
    "    Implementation of Standard Scaler to scale the features. The features are scaled similar to how `sklearn.preprocessing.StandardScaler` works\n",
    "\n",
    "    Args:\n",
    "        data: Data to Normalize\n",
    "    \n",
    "    Returns:\n",
    "        A numpy array containing the scaled features and the mean and standard deviation used for scaling\n",
    "    \"\"\"\n",
    "    return (data - data.mean(axis=0)) / data.std(axis=0), data.mean(axis=0), data.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "e889e293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 5)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "b46436b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.73059361, 0.00951267, 1.        , 0.61694135, 0.71932284],\n",
       "        [0.44520548, 0.04380939, 0.9       , 0.5849491 , 0.71145137],\n",
       "        [0.30365297, 0.08331505, 0.5       , 0.67123122, 0.75889584],\n",
       "        ...,\n",
       "        [0.42922374, 0.05686115, 0.7       , 0.57149782, 0.71522536],\n",
       "        [0.18493151, 0.0125958 , 0.5       , 0.42014057, 0.72395946],\n",
       "        [0.14840183, 0.0103754 , 0.9       , 0.51211827, 0.75016174]]),\n",
       " array([  0.     ,  23.38284,   0.     ,  24.93207, 121.47353]),\n",
       " array([  43.8    , 6488.021  ,   10.     ,   25.01459,  121.56627]))"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_normalization(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "98128952",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "dbce4619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Fold Cross Validation with k=3\n"
     ]
    }
   ],
   "source": [
    "folds = k_fold_cross_validation(X, y, k, random_seed=0)\n",
    "print(f\"K-Fold Cross Validation with k={k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "8d2378d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalization(data):\n",
    "    \"\"\"\n",
    "    Implementation of min-max normalization to scale the features. The features are scaled similar to how `sklearn.preprocessing.MinMaxScaler` works\n",
    "\n",
    "    Args:\n",
    "        data: Data to Normalize\n",
    "    \n",
    "    Returns:\n",
    "        A numpy array containing the scaled features and the min and max values used for scaling\n",
    "    \"\"\"\n",
    "    normalized_data = (data - np.amin(data,axis=0))/(np.amax(data,axis=0) - np.amin(data,axis=0))\n",
    "    return (normalized_data, np.amin(data,axis=0), np.amax(data,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "063faa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.50684932 0.05509584 1.         0.62239457 0.69107181]\n",
      " [1.         0.02283105 0.0263281  0.6        0.40765875 0.72633168]\n",
      " [1.         0.30136986 0.07252509 0.5        0.40087252 0.68837611]\n",
      " ...\n",
      " [1.         0.31050228 0.64566122 0.         0.0821619  0.32671986]\n",
      " [1.         0.8196347  0.09549742 0.3        0.52787203 0.68600388]\n",
      " [1.         0.15068493 0.0103754  0.9        0.51211827 0.75016174]]\n"
     ]
    }
   ],
   "source": [
    "for i, (x_train, y_train, x_val, y_val) in enumerate(folds):\n",
    "    x_train, normalizer_min, normalizer_max = min_max_normalization(x_train)\n",
    "    x_val = (x_val - normalizer_min) / (normalizer_max - normalizer_min)\n",
    "    x_train = np.append([[1]]*len(x_train), x_train, axis=1)\n",
    "    x_val = np.append([[1]]*len(x_val), x_val, axis=1)\n",
    "    print(x_train)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "908247cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_solve_normal_form(x_train, y_train, x_val, y_val):\n",
    "    \"\"\"\n",
    "    Implementation of Linear Regression using Normal Form\n",
    "\n",
    "    Args:\n",
    "        x_train: Training Data\n",
    "        y_train: Training Labels\n",
    "        x_val: Validation Data\n",
    "        y_val: Validation Labels\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the parameters, bias, training RMSE, validation RMSE\n",
    "    \"\"\"\n",
    "    x_train, normalizer_min, normalizer_max = min_max_normalization(x_train)\n",
    "    x_val = (x_val - normalizer_min) / (normalizer_max - normalizer_min)\n",
    "    x_train = np.append([[1]]*len(x_train), x_train, axis=1)\n",
    "    x_val = np.append([[1]]*len(x_val), x_val, axis=1)\n",
    "    params = np.linalg.inv(x_train.T.dot(x_train)).dot(x_train.T).dot(y_train)\n",
    "    y_train_pred = x_train.dot(params)\n",
    "    y_val_pred = x_val.dot(params)\n",
    "    train_RMSE = RMSE(y_train, y_train_pred)\n",
    "    val_RMSE = RMSE(y_val, y_val_pred)\n",
    "    return params, bias, train_RMSE, val_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "eb0b1bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "13.026685393136084 14.498455357565478\n",
      "Fold 2\n",
      "13.607053004795615 13.6787023860842\n",
      "Fold 3\n",
      "13.761795677691543 13.098323935342542\n"
     ]
    }
   ],
   "source": [
    "for i, (x_train, y_train, x_val, y_val) in enumerate(folds):\n",
    "    print(f\"Fold {i+1}\")\n",
    "    params, bias, train_RMSE_for_normal_equation, val_RMSE_for_normal_equation = linear_regression_solve_normal_form(x_train, y_train, x_val, y_val)\n",
    "    print(train_RMSE_for_normal_equation, val_RMSE_for_normal_equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "62ebbdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "(276, 6)\n",
      "(6,)\n",
      "(138, 6)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (276,6) (276,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [311]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x_train, y_train, x_val, y_val) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(folds):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     params, ls_RMSE_train, ls_RMSE_val \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_regression_with_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mregularization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularization_scaling_factor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43museEarlyStopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mEarlyStopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEarlyStopping_min_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ls_RMSE_train[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], ls_RMSE_val[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "Input \u001b[1;32mIn [299]\u001b[0m, in \u001b[0;36mlinear_regression_with_gradient_descent\u001b[1;34m(x_train, y_train, x_val, y_val, epochs, lr, normalize, regularization, regularization_scaling_factor, useEarlyStopping, EarlyStopping_patience, EarlyStopping_min_delta, random_seed, verbose)\u001b[0m\n\u001b[0;32m     65\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x_train, params)\n\u001b[0;32m     66\u001b[0m         loss \u001b[38;5;241m=\u001b[39m ComputeLoss(y_train, y_pred, params, bias, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL2\u001b[39m\u001b[38;5;124m\"\u001b[39m, regularization, regularization_scaling_factor)\n\u001b[1;32m---> 67\u001b[0m         params[i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(x_train)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m#         for i in range(num_params):\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m#             if regularization == 'No':\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m#                 params[i] -= lr * np.sum(x_train[:, i] * (y_pred - y_train))/len(x_train)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m#             elif regularization == 'Ridge':\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m#                 params[i] -= lr * (np.sum(x_train[:, i] * (y_pred - y_train))/len(x_train) + regularization_scaling_factor*2*params[i]) \u001b[39;00m\n\u001b[0;32m     76\u001b[0m         y_pred_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x_val, params)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (276,6) (276,) "
     ]
    }
   ],
   "source": [
    "for i, (x_train, y_train, x_val, y_val) in enumerate(folds):\n",
    "    print(f\"Fold {i+1}\")\n",
    "    params, ls_RMSE_train, ls_RMSE_val = linear_regression_with_gradient_descent(x_train, y_train, x_val, y_val, epochs=1000, lr=0.01, \n",
    "                                                            regularization='No', regularization_scaling_factor = 'NA', useEarlyStopping=True,\n",
    "                                                            EarlyStopping_patience=5, EarlyStopping_min_delta=1e-1,\n",
    "                                                            random_seed=42, verbose=3)\n",
    "    print(ls_RMSE_train[-1], ls_RMSE_val[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "aecd54f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   4.     , 2147.376  ,    3.     ,   24.96299,  121.51284,\n",
       "           5.     ],\n",
       "       [   3.1    ,  383.8624 ,    5.     ,   24.98085,  121.54391,\n",
       "           6.     ],\n",
       "       [   4.1    ,   56.47425,    7.     ,   24.95744,  121.53711,\n",
       "           5.     ],\n",
       "       ...,\n",
       "       [   6.6    ,   90.45606,    9.     ,   24.97433,  121.5431 ,\n",
       "           5.     ],\n",
       "       [  20.6    , 2469.645  ,    4.     ,   24.96108,  121.51046,\n",
       "           6.     ],\n",
       "       [  20.3    ,  287.6025 ,    6.     ,   24.98042,  121.54228,\n",
       "           5.     ]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "09acbe6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 4.0000000e+00, 2.1473760e+03, ..., 2.4962990e+01,\n",
       "        1.2151284e+02, 5.0000000e+00],\n",
       "       [1.0000000e+00, 3.1000000e+00, 3.8386240e+02, ..., 2.4980850e+01,\n",
       "        1.2154391e+02, 6.0000000e+00],\n",
       "       [1.0000000e+00, 4.1000000e+00, 5.6474250e+01, ..., 2.4957440e+01,\n",
       "        1.2153711e+02, 5.0000000e+00],\n",
       "       ...,\n",
       "       [1.0000000e+00, 6.6000000e+00, 9.0456060e+01, ..., 2.4974330e+01,\n",
       "        1.2154310e+02, 5.0000000e+00],\n",
       "       [1.0000000e+00, 2.0600000e+01, 2.4696450e+03, ..., 2.4961080e+01,\n",
       "        1.2151046e+02, 6.0000000e+00],\n",
       "       [1.0000000e+00, 2.0300000e+01, 2.8760250e+02, ..., 2.4980420e+01,\n",
       "        1.2154228e+02, 5.0000000e+00]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582e9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
