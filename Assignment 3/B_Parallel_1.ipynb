{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "npFemIWusYQi"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oTrTMpTwtLXd"
      },
      "outputs": [],
      "source": [
        "class AffineTransformationLayer:\n",
        "    def __init__(self, input_size, output_size, weight_initializer, random_init_scaler=True):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        if weight_initializer == 'zeros':\n",
        "            self.weights = np.zeros((input_size, output_size))\n",
        "            self.bias = np.zeros((1, output_size))\n",
        "        elif weight_initializer == 'random_init':\n",
        "            self.weights = np.random.random((input_size, output_size))\n",
        "            self.bias = np.random.random((1, output_size))\n",
        "            if random_init_scaler:\n",
        "                self.weights = self.weights / np.sqrt(input_size + output_size)\n",
        "                self.bias = self.bias / np.sqrt(input_size + output_size)\n",
        "        elif weight_initializer == 'normal_init':\n",
        "            self.weights = np.random.normal(0, 0.1, (input_size, output_size))\n",
        "            self.bias = np.random.normal(0, 0.1, (1, output_size))\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.dot(input, self.weights) + self.bias\n",
        "\n",
        "    def backward(self, output_error, learning_rate):\n",
        "        input_error = np.dot(output_error, self.weights.T)\n",
        "        weights_error = np.dot(self.input.T, output_error)\n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.bias -= learning_rate * output_error\n",
        "        return input_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E6nSYAB2sam3"
      },
      "outputs": [],
      "source": [
        "class ActivationLayer:\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "    \n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return self.activation(input)\n",
        "    \n",
        "    def backward(self, output_error, learning_rate):\n",
        "        return output_error * self.activation_prime(self.input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hl8LxP1lAEiN"
      },
      "outputs": [],
      "source": [
        "class Reshaping_Layer:\n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "    def forward(self, input):\n",
        "        return np.reshape(input, (1, -1))\n",
        "    \n",
        "    def backward(self, output_error, learning_rate):\n",
        "        return np.reshape(output_error, self.input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RQeuIfkK3vyl"
      },
      "outputs": [],
      "source": [
        "class SoftmaxLayer:\n",
        "    def __init__(self, input_size):\n",
        "        self.input_size = input_size\n",
        "    \n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        tmp = np.exp(input)\n",
        "        self.output = tmp / np.sum(tmp)\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, output_error, learning_rate):\n",
        "        input_error = np.zeros(output_error.shape)\n",
        "        out = np.tile(self.output.T, self.input_size)\n",
        "        return self.output * np.dot(output_error, np.identity(self.input_size) - out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LuPbn70Wt8Q7"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    return np.exp(-x) / (1 + np.exp(-x))**2\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_prime(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def relu_prime(x):\n",
        "    return np.array(x >= 0).astype('int')\n",
        "\n",
        "def leaky_relu(x):\n",
        "    return np.maximum(x, 0.01 * x)\n",
        "\n",
        "def leaky_relu_prime(x):\n",
        "    return np.array(x >= 0).astype('int') + 0.01 * np.array(x < 0).astype('int')\n",
        "\n",
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "def linear_prime(x):\n",
        "    return np.ones(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rXY7jkUzuqEk"
      },
      "outputs": [],
      "source": [
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / y_pred.size\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "def mae_prime(y_true, y_pred):\n",
        "    return np.sign(y_pred - y_true) / y_pred.size\n",
        "\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "def cross_entropy_prime(y_true, y_pred):\n",
        "    return -y_true / y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "-whGNp8Joaur",
        "outputId": "23690d5d-f451-4fd3-b87a-d5c416ac2d0b"
      },
      "outputs": [],
      "source": [
        "from utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_mnist()\n",
        "\n",
        "# Normalize the images.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "x_train = x_train[:10000]\n",
        "y_train = y_train[:10000]\n",
        "\n",
        "# Flatten the images.\n",
        "x_train = x_train.reshape((-1, 784))\n",
        "x_test = x_test.reshape((-1, 784))\n",
        "\n",
        "# One hot encode the labels.\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Split the train data into train and validation sets.\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, n_inputs,  input_shape, hidden_layer_sizes, n_outs, activation_fn, activation_fn_derivative, weight_initialization, loss, loss_prime):\n",
        "        self.network = []\n",
        "        self.network.append(Reshaping_Layer(input_shape))\n",
        "        # Input To Hidden\n",
        "        self.network.append(AffineTransformationLayer(n_inputs, hidden_layer_sizes[0], weight_initialization))\n",
        "        self.network.append(ActivationLayer(activation_fn, activation_fn_derivative))\n",
        "        # Hidden To Hidden\n",
        "        for i in range(len(hidden_layer_sizes) - 1):\n",
        "            self.network.append(AffineTransformationLayer(hidden_layer_sizes[i], hidden_layer_sizes[i+1], weight_initialization))\n",
        "            self.network.append(ActivationLayer(activation_fn, activation_fn_derivative))\n",
        "        # Hidden To Output\n",
        "        self.network.append(AffineTransformationLayer(hidden_layer_sizes[-1], n_outs, weight_initialization))\n",
        "        self.network.append(SoftmaxLayer(n_outs))\n",
        "        self.loss = loss\n",
        "        self.loss_prime = loss_prime\n",
        "        self.train_loss = []\n",
        "        self.val_loss = []\n",
        "    \n",
        "    def predict(self, x):\n",
        "        for layer in self.network:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "    \n",
        "    def fit(self, x_train, y_train, x_valid, y_valid, epochs=1000, learning_rate=0.1, batch_size=32, early_stopping=True, patience=10, verbose=True):\n",
        "        x_batches_train = np.array_split(x_train, len(x_train)/ batch_size)\n",
        "        y_batches_train = np.array_split(y_train, len(y_train)/ batch_size)\n",
        "        for epoch in range(epochs):\n",
        "            error = 0\n",
        "            error_val = 0\n",
        "            for x_train, y_train in zip(x_batches_train,y_batches_train):\n",
        "                for x, y_true in zip(x_train, y_train):\n",
        "\n",
        "                    output = x\n",
        "                    \n",
        "                    output = self.forward(output)\n",
        "\n",
        "                    error += self.loss(y_true, output)\n",
        "\n",
        "                    output_error = self.loss_prime(y_true, output)\n",
        "\n",
        "                    self.backward(output_error, learning_rate)\n",
        "\n",
        "                    error /= len(x_train)\n",
        "\n",
        "            error_val = sum([self.loss(y, self.predict(x)) for x, y in zip(x_valid, y_valid)]) / len(x_valid) \n",
        "\n",
        "            self.train_loss.append(error)\n",
        "            self.val_loss.append(error_val)\n",
        "\n",
        "            if early_stopping:\n",
        "                if epoch > patience:\n",
        "                    if self.train_loss[-1] > np.mean(self.val_loss[-(patience+1):-1]):\n",
        "                        print('Early Stopping')\n",
        "                        break\n",
        "            if verbose:\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f\"Epoch: {epoch}, Training Loss: {error}, Validation Loss: {error_val}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.network:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, output_error, learning_rate):\n",
        "        for layer in reversed(self.network):\n",
        "            output_error = layer.backward(output_error, learning_rate)\n",
        "\n",
        "    def predict_proba(self, x):\n",
        "        return self.predict(x)\n",
        "\n",
        "    def score(self, x, y):\n",
        "        cnt = 0\n",
        "        for x_v, y_v in zip(x,y):\n",
        "            op = self.predict(x_v)\n",
        "            if (np.argmax(op) == np.argmax(y_v)):\n",
        "                cnt+=1\n",
        "        return cnt/y.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model(model, filename):\n",
        "    import pickle\n",
        "    with open(f\"Model_Saves/{filename}\", 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "def load_model(filename):\n",
        "    import pickle\n",
        "    with open(f\"Model_Saves/{filename}\", 'rb') as f:\n",
        "        return pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((8000, 784), (8000, 10), (2000, 784), (2000, 10))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "activation_fns = [relu, sigmoid, tanh, leaky_relu, linear]\n",
        "activation_fns_prime = [relu_prime, sigmoid_prime, tanh_prime, leaky_relu_prime, linear_prime]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Activation Function: relu\n",
            "Epoch: 0, Training Loss: 0.0006926952076625672, Validation Loss: 0.08987083738288432\n",
            "Epoch: 10, Training Loss: 0.0004886420801814411, Validation Loss: 0.08025524917728491\n",
            "Epoch: 20, Training Loss: 0.0002910827704302052, Validation Loss: 0.04729564473916273\n",
            "Epoch: 30, Training Loss: 0.00022309467777434735, Validation Loss: 0.026681221408507413\n",
            "Epoch: 40, Training Loss: 0.0002483548818066395, Validation Loss: 0.020125978557046443\n",
            "Epoch: 50, Training Loss: 0.00019106551210929955, Validation Loss: 0.017290346330173952\n",
            "Epoch: 60, Training Loss: 0.00014229941190942362, Validation Loss: 0.01581425884628157\n",
            "Epoch: 70, Training Loss: 0.00011446359812789067, Validation Loss: 0.014881210616549763\n",
            "Epoch: 80, Training Loss: 0.00010140715103460584, Validation Loss: 0.0142126615987349\n",
            "Epoch: 90, Training Loss: 9.370510232731414e-05, Validation Loss: 0.013680512473825214\n",
            "Accuracy: 0.9163\n",
            "Activation Function: sigmoid\n",
            "Epoch: 0, Training Loss: 0.0007028043590856087, Validation Loss: 0.09033084009539188\n",
            "Epoch: 10, Training Loss: 0.0006870606705666409, Validation Loss: 0.08997198009203561\n",
            "Epoch: 20, Training Loss: 0.0006857608353638025, Validation Loss: 0.08993751335984543\n",
            "Epoch: 30, Training Loss: 0.0006855642315335144, Validation Loss: 0.08993261454837514\n",
            "Epoch: 40, Training Loss: 0.0006855055788113024, Validation Loss: 0.08993131042969929\n",
            "Epoch: 50, Training Loss: 0.0006854808599263695, Validation Loss: 0.08993061316175546\n",
            "Epoch: 60, Training Loss: 0.0006854658901593149, Validation Loss: 0.0899300322500692\n",
            "Epoch: 70, Training Loss: 0.0006854537242472195, Validation Loss: 0.0899294728801845\n",
            "Epoch: 80, Training Loss: 0.0006854423581642934, Validation Loss: 0.08992891605566412\n",
            "Epoch: 90, Training Loss: 0.0006854312139643406, Validation Loss: 0.08992835794203721\n",
            "Accuracy: 0.1135\n",
            "Activation Function: tanh\n",
            "Epoch: 0, Training Loss: 0.0006897302587329442, Validation Loss: 0.0886117196136765\n",
            "Epoch: 10, Training Loss: 0.0003197364866051921, Validation Loss: 0.064729394470559\n",
            "Epoch: 20, Training Loss: 0.00018152332983309414, Validation Loss: 0.04256412630087584\n",
            "Epoch: 30, Training Loss: 0.00022917305214622017, Validation Loss: 0.02910468542299548\n",
            "Epoch: 40, Training Loss: 0.00025443253695855533, Validation Loss: 0.022134685528934855\n",
            "Epoch: 50, Training Loss: 0.0002464663768961053, Validation Loss: 0.018734626731456677\n",
            "Epoch: 60, Training Loss: 0.00023198805587594997, Validation Loss: 0.016800671482719822\n",
            "Epoch: 70, Training Loss: 0.00021737177753660357, Validation Loss: 0.015547892733599675\n",
            "Epoch: 80, Training Loss: 0.00020328791200093389, Validation Loss: 0.01465901425916929\n",
            "Epoch: 90, Training Loss: 0.0001896748515675393, Validation Loss: 0.013989888894710376\n",
            "Accuracy: 0.9193\n",
            "Activation Function: leaky_relu\n",
            "Epoch: 0, Training Loss: 0.0006743239911517778, Validation Loss: 0.08979612747593203\n",
            "Epoch: 10, Training Loss: 0.0004024039266254769, Validation Loss: 0.07099536056408887\n",
            "Epoch: 20, Training Loss: 0.0002179885470383659, Validation Loss: 0.03642981299121815\n",
            "Epoch: 30, Training Loss: 0.0003146168685909974, Validation Loss: 0.02313747886618535\n",
            "Epoch: 40, Training Loss: 0.0002918089960807881, Validation Loss: 0.01879637056616241\n",
            "Epoch: 50, Training Loss: 0.0002480701587689011, Validation Loss: 0.01683037247831594\n",
            "Epoch: 60, Training Loss: 0.00020860700805482924, Validation Loss: 0.01567966582472838\n",
            "Epoch: 70, Training Loss: 0.00017528010944164795, Validation Loss: 0.014890353437518553\n",
            "Epoch: 80, Training Loss: 0.00014732761701407576, Validation Loss: 0.014320205966779121\n",
            "Epoch: 90, Training Loss: 0.00012695758202570898, Validation Loss: 0.013858240229925198\n",
            "Accuracy: 0.9138\n",
            "Activation Function: linear\n",
            "Epoch: 0, Training Loss: 0.0006458037415089318, Validation Loss: 0.0807322028261389\n",
            "Epoch: 10, Training Loss: 0.0004199118935458075, Validation Loss: 0.022657710890888166\n",
            "Epoch: 20, Training Loss: 0.00045185691025874326, Validation Loss: 0.018081298931095034\n",
            "Epoch: 30, Training Loss: 0.0004831991901376458, Validation Loss: 0.01659771596960642\n",
            "Epoch: 40, Training Loss: 0.0004981089933032137, Validation Loss: 0.015940036632075806\n",
            "Epoch: 50, Training Loss: 0.0005016422187791602, Validation Loss: 0.015585299537613515\n",
            "Epoch: 60, Training Loss: 0.0004964404390165194, Validation Loss: 0.015356901035035795\n",
            "Epoch: 70, Training Loss: 0.0004803510718564548, Validation Loss: 0.015200557689564949\n",
            "Epoch: 80, Training Loss: 0.0004557376553844768, Validation Loss: 0.015116957581298805\n",
            "Epoch: 90, Training Loss: 0.0004301039691733229, Validation Loss: 0.015090853986696434\n",
            "Accuracy: 0.9056\n"
          ]
        }
      ],
      "source": [
        "for activation_fn, activation_fn_prime in zip(activation_fns, activation_fns_prime):\n",
        "    print(f\"Activation Function: {activation_fn.__name__}\")\n",
        "    model = NeuralNetwork(784, (28,28), [256,128,64,32], 10, activation_fn, activation_fn_prime, 'normal_init', mse, mse_prime)\n",
        "    model.fit(x_train, y_train, x_val, y_val, epochs=100, learning_rate=0.001, batch_size=128, early_stopping=True, patience=10, verbose=True)\n",
        "    print(f\"Accuracy: {model.score(x_test, y_test)}\")\n",
        "    save_model(model, f\"model_{activation_fn.__name__}.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "initialization_fns = ['zeros', 'normal_init', 'random_init']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Training Loss: 0.0022557183192438964, Validation Loss: 0.08022426883322133\n",
            "Epoch: 1, Training Loss: 0.0018364025694024208, Validation Loss: 0.07063569194875897\n",
            "Epoch: 2, Training Loss: 0.0015514003335942873, Validation Loss: 0.06413582115634027\n",
            "Epoch: 3, Training Loss: 0.001321268683615486, Validation Loss: 0.05958066708476712\n",
            "Epoch: 4, Training Loss: 0.0011662841823164183, Validation Loss: 0.05586707933478441\n",
            "Epoch: 5, Training Loss: 0.0010235125091393783, Validation Loss: 0.05259582749877764\n",
            "Epoch: 6, Training Loss: 0.0009198648657909622, Validation Loss: 0.04968066652503007\n",
            "Epoch: 7, Training Loss: 0.0008633215877789182, Validation Loss: 0.047105525658676116\n",
            "Epoch: 8, Training Loss: 0.0008442904916260939, Validation Loss: 0.0448600442300863\n",
            "Epoch: 9, Training Loss: 0.0008036938675641232, Validation Loss: 0.04287671746424573\n",
            "Epoch: 10, Training Loss: 0.0007692231982143351, Validation Loss: 0.04111311313056513\n",
            "Epoch: 11, Training Loss: 0.0007323039390737545, Validation Loss: 0.03952451572109199\n",
            "Epoch: 12, Training Loss: 0.000707842502131007, Validation Loss: 0.03808465524228426\n",
            "Epoch: 13, Training Loss: 0.0006763203399251056, Validation Loss: 0.036796899002767565\n",
            "Epoch: 14, Training Loss: 0.0006682468707598688, Validation Loss: 0.035630394031457666\n",
            "Epoch: 15, Training Loss: 0.000658321238468939, Validation Loss: 0.03459509677595208\n",
            "Epoch: 16, Training Loss: 0.0006283902712745898, Validation Loss: 0.033649709871560546\n",
            "Epoch: 17, Training Loss: 0.0006138072486628292, Validation Loss: 0.032783453265798186\n",
            "Epoch: 18, Training Loss: 0.0005909389331395865, Validation Loss: 0.03198239173177207\n",
            "Epoch: 19, Training Loss: 0.000574855392802273, Validation Loss: 0.0312554875808\n",
            "Epoch: 20, Training Loss: 0.0005489062551617952, Validation Loss: 0.03057823072801733\n",
            "Epoch: 21, Training Loss: 0.0005203213421854609, Validation Loss: 0.02994958952772487\n",
            "Epoch: 22, Training Loss: 0.00047832667748123236, Validation Loss: 0.02934231909348241\n",
            "Epoch: 23, Training Loss: 0.0004784184676443634, Validation Loss: 0.028731561773484057\n",
            "Epoch: 24, Training Loss: 0.00047865490706447, Validation Loss: 0.028134848475073006\n",
            "Epoch: 25, Training Loss: 0.000466452203882083, Validation Loss: 0.027603170951582746\n",
            "Epoch: 26, Training Loss: 0.00045067437212449775, Validation Loss: 0.027106225986266684\n",
            "Epoch: 27, Training Loss: 0.00043740099995032584, Validation Loss: 0.026645404538939373\n",
            "Epoch: 28, Training Loss: 0.00042746536374661814, Validation Loss: 0.026210343430440555\n",
            "Epoch: 29, Training Loss: 0.000416549596012257, Validation Loss: 0.02579756303929707\n",
            "Epoch: 30, Training Loss: 0.00039790833419402904, Validation Loss: 0.025405804821424842\n",
            "Epoch: 31, Training Loss: 0.0003848043714196578, Validation Loss: 0.025036636235284945\n",
            "Epoch: 32, Training Loss: 0.00037219443545358905, Validation Loss: 0.024685602954661247\n",
            "Epoch: 33, Training Loss: 0.0003574933869364374, Validation Loss: 0.024351642550436012\n",
            "Epoch: 34, Training Loss: 0.0003441679542610709, Validation Loss: 0.024036473942916543\n",
            "Epoch: 35, Training Loss: 0.0003327781257106146, Validation Loss: 0.0237368066903668\n",
            "Epoch: 36, Training Loss: 0.0003229906129879932, Validation Loss: 0.02345410572647367\n",
            "Epoch: 37, Training Loss: 0.0003098709695326552, Validation Loss: 0.02318022743828094\n",
            "Epoch: 38, Training Loss: 0.00028934774153334945, Validation Loss: 0.022914411796515075\n",
            "Epoch: 39, Training Loss: 0.00027621280853133245, Validation Loss: 0.02266757006250642\n",
            "Epoch: 40, Training Loss: 0.00026048720400077054, Validation Loss: 0.022431931286985448\n",
            "Epoch: 41, Training Loss: 0.0002466165032128141, Validation Loss: 0.022203919573146415\n",
            "Epoch: 42, Training Loss: 0.00023178267507652255, Validation Loss: 0.02198552115775756\n",
            "Epoch: 43, Training Loss: 0.00022244305628221, Validation Loss: 0.02177500309923928\n",
            "Epoch: 44, Training Loss: 0.00021225196760634878, Validation Loss: 0.02157486204569702\n",
            "Epoch: 45, Training Loss: 0.0002049944679757182, Validation Loss: 0.021380340914496792\n",
            "Epoch: 46, Training Loss: 0.0001992771819766228, Validation Loss: 0.021191929970410563\n",
            "Epoch: 47, Training Loss: 0.0001923718482751275, Validation Loss: 0.02100804151099745\n",
            "Epoch: 48, Training Loss: 0.0001859190974616328, Validation Loss: 0.02083072464549397\n",
            "Epoch: 49, Training Loss: 0.00017938550727571165, Validation Loss: 0.020660210810999557\n",
            "Epoch: 50, Training Loss: 0.0001748673312481713, Validation Loss: 0.020496231738535067\n",
            "Epoch: 51, Training Loss: 0.00016855253117930912, Validation Loss: 0.02033636125772045\n",
            "Epoch: 52, Training Loss: 0.00016457366578581336, Validation Loss: 0.020180262275336407\n",
            "Epoch: 53, Training Loss: 0.00015940936602274367, Validation Loss: 0.02003116133132583\n",
            "Epoch: 54, Training Loss: 0.00015499698712489283, Validation Loss: 0.019890927425780622\n",
            "Epoch: 55, Training Loss: 0.00014900174190433013, Validation Loss: 0.01975216795362115\n",
            "Epoch: 56, Training Loss: 0.00014365556786556934, Validation Loss: 0.019618382283750638\n",
            "Epoch: 57, Training Loss: 0.00013830796996513802, Validation Loss: 0.019486346901630012\n",
            "Epoch: 58, Training Loss: 0.00013410112581438362, Validation Loss: 0.01935925764194445\n",
            "Epoch: 59, Training Loss: 0.00012994683342413738, Validation Loss: 0.019234200046342107\n",
            "Epoch: 60, Training Loss: 0.00012627915757758068, Validation Loss: 0.019111160113594962\n",
            "Epoch: 61, Training Loss: 0.00012276138453133124, Validation Loss: 0.01899425451472275\n",
            "Epoch: 62, Training Loss: 0.00012007445266267684, Validation Loss: 0.01888214414570863\n",
            "Epoch: 63, Training Loss: 0.00011682689599595494, Validation Loss: 0.018771759937197714\n",
            "Epoch: 64, Training Loss: 0.0001143085852795447, Validation Loss: 0.018664115731076816\n",
            "Epoch: 65, Training Loss: 0.0001119982757350993, Validation Loss: 0.01855933801976339\n",
            "Epoch: 66, Training Loss: 0.00010975435484154091, Validation Loss: 0.01845743806820258\n",
            "Epoch: 67, Training Loss: 0.00010846543486554756, Validation Loss: 0.018354822275973216\n",
            "Epoch: 68, Training Loss: 0.00010643092822598519, Validation Loss: 0.01825359552923499\n",
            "Epoch: 69, Training Loss: 0.00010473120956296152, Validation Loss: 0.0181584721807446\n",
            "Epoch: 70, Training Loss: 0.00010305671922275191, Validation Loss: 0.018060105299712366\n",
            "Epoch: 71, Training Loss: 0.00010211116486811665, Validation Loss: 0.017962185125453847\n",
            "Epoch: 72, Training Loss: 0.00010209875170715034, Validation Loss: 0.017872069261695843\n",
            "Epoch: 73, Training Loss: 0.00010071482012426974, Validation Loss: 0.017783898327084324\n",
            "Epoch: 74, Training Loss: 0.00010009410472326783, Validation Loss: 0.017692475204991596\n",
            "Epoch: 75, Training Loss: 9.958189209484834e-05, Validation Loss: 0.017604908269097296\n",
            "Epoch: 76, Training Loss: 9.902100086992911e-05, Validation Loss: 0.017516902675921802\n",
            "Epoch: 77, Training Loss: 9.870636323381604e-05, Validation Loss: 0.017430074960077584\n",
            "Epoch: 78, Training Loss: 9.861865672327358e-05, Validation Loss: 0.017344015937150067\n",
            "Epoch: 79, Training Loss: 9.840507886689132e-05, Validation Loss: 0.0172592305726705\n",
            "Epoch: 80, Training Loss: 9.881317279705134e-05, Validation Loss: 0.017178615383596572\n",
            "Epoch: 81, Training Loss: 9.977688210492e-05, Validation Loss: 0.01709914369747582\n",
            "Epoch: 82, Training Loss: 0.000100685876955096, Validation Loss: 0.01702306297632578\n",
            "Epoch: 83, Training Loss: 0.00010124680274200054, Validation Loss: 0.016945811607999132\n",
            "Epoch: 84, Training Loss: 0.00010235181874966096, Validation Loss: 0.016870897106359653\n",
            "Epoch: 85, Training Loss: 0.00010240556549267591, Validation Loss: 0.016798101705569606\n",
            "Epoch: 86, Training Loss: 0.00010253548807050821, Validation Loss: 0.016726031014180184\n",
            "Epoch: 87, Training Loss: 0.00010264739761197092, Validation Loss: 0.01665482649989254\n",
            "Epoch: 88, Training Loss: 0.00010268488125698729, Validation Loss: 0.01658590135742803\n",
            "Epoch: 89, Training Loss: 0.00010221787022468855, Validation Loss: 0.016530413281272346\n",
            "Epoch: 90, Training Loss: 0.00010207582592272109, Validation Loss: 0.01646182776645708\n",
            "Epoch: 91, Training Loss: 0.00010257750729433184, Validation Loss: 0.01639489788122222\n",
            "Epoch: 92, Training Loss: 0.00010316349554757627, Validation Loss: 0.016328941911460646\n",
            "Epoch: 93, Training Loss: 0.00010295888081019862, Validation Loss: 0.016264612522141505\n",
            "Epoch: 94, Training Loss: 0.00010391745272479608, Validation Loss: 0.016199544077027766\n",
            "Epoch: 95, Training Loss: 0.00010470058911953351, Validation Loss: 0.0161368555707558\n",
            "Epoch: 96, Training Loss: 0.00010592669441183355, Validation Loss: 0.016075456266553233\n",
            "Epoch: 97, Training Loss: 0.00010617754775808872, Validation Loss: 0.016015845400329833\n",
            "Epoch: 98, Training Loss: 0.00010692758023087665, Validation Loss: 0.015957155986639618\n",
            "Epoch: 99, Training Loss: 0.00010670164452139273, Validation Loss: 0.015898240746698784\n"
          ]
        }
      ],
      "source": [
        "for initialization_fn in initialization_fns:\n",
        "    print(f\"Initialization Function: {initialization_fn}\")\n",
        "    model = NeuralNetwork(784, (28,28), [256,128,64,32], 10, relu, relu_prime, initialization_fn, mse, mse_prime)\n",
        "    model.fit(x_train, y_train, x_val, y_val, epochs=50, learning_rate=0.001, batch_size=128, early_stopping=True, patience=10, verbose=True)\n",
        "    print(f\"Accuracy: {model.score(x_test, y_test)}\")\n",
        "    save_model(model, f\"model_{initialization_fn}_activation_relu.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.933100\n"
          ]
        }
      ],
      "source": [
        "# Load Relu Model\n",
        "relu_model = load_model(\"model_relu.pkl\")\n",
        "# Load Sigmoid Model\n",
        "sigmoid_model = load_model(\"model_sigmoid.pkl\")\n",
        "# Load Tanh Model\n",
        "tanh_model = load_model(\"model_tanh.pkl\")\n",
        "# Load Leaky Relu Model\n",
        "leaky_relu_model = load_model(\"model_leaky_relu.pkl\")\n",
        "# Load Linear Model\n",
        "linear_model = load_model(\"model_linear.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAqnUlEQVR4nO3de3xV5Z3v8c8vdwgQQkAFAgYFlfstIh2qqLQOtFW0xYLVqe2xZeypx+npTKdM54xtPafn6BmPt9a2Q6uOta3IMNVitaVjgWI7SgkWEQSUmxJACZeEa4Akv/PHs0J2QkI25LKTle/79VqvvS7P2uu3XPh7Vp619vOYuyMiIvGVluoARESkbSnRi4jEnBK9iEjMKdGLiMScEr2ISMxlpDqAhvr27etFRUWpDkNEpFNZvXr1Xnfv19i2Dpfoi4qKKCkpSXUYIiKdipm929Q2Nd2IiMScEr2ISMwp0YuIxFyHa6MXkfZ38uRJSktLqaysTHUo0oycnBwKCwvJzMxMeh8lehGhtLSUnj17UlRUhJmlOhxpgruzb98+SktLGTJkSNL7qelGRKisrKSgoEBJvoMzMwoKCs76L6+kEr2ZTTezTWa22czmNbI928yejbavNLOiaH2mmT1lZm+a2QYz+4ezik5E2o2SfOdwLtep2URvZunAY8AMYARwi5mNaFDsDuCAuw8FHgLuj9bfDGS7+2hgIvDXtZVAqyvfAUu/A/u3tsnXi4h0Vsnc0U8CNrv7Vnc/ASwAZjYoMxN4KppfBEyzUO04kGtmGUA34ARwsFUib6iyHFb8X9j9Rpt8vYi0nfLycr7//e+f074f+9jHKC8vT7r8t771LR544IFzOlZnlUyiHwjsSFgujdY1Wsbdq4AKoICQ9I8Au4H3gAfcfX/DA5jZXDMrMbOSsrKysz4JAHpfGD4PbD+3/UUkZc6U6Kuqqs6470svvUTv3r3bIKr4aOuHsZOAamAAMAT4WzO7qGEhd5/v7sXuXtyvX6NdNTQvpxd06wMHmvwVsIh0UPPmzWPLli2MGzeOr33tayxfvpwrr7ySG264gREjQkvxjTfeyMSJExk5ciTz588/tW9RURF79+5l+/btDB8+nC9+8YuMHDmS6667jmPHjp3xuGvWrGHy5MmMGTOGm266iQMHDgDw6KOPMmLECMaMGcOcOXMA+P3vf8+4ceMYN24c48eP59ChQ230X6P1JfN65U5gUMJyYbSusTKlUTNNHrAP+AzwG3c/Cewxsz8CxUDbNKTnXwjlSvQiLfHtF9bz1q7WbWEdMaAX37x+ZJPb77vvPtatW8eaNWsAWL58Oa+//jrr1q079RrhE088QZ8+fTh27BiXX345n/rUpygoKKj3Pe+88w7PPPMMP/rRj/j0pz/Nv//7v3Pbbbc1edzPfvazfPe732Xq1Kncc889fPvb3+bhhx/mvvvuY9u2bWRnZ59qFnrggQd47LHHmDJlCocPHyYnJ6dl/1HaUTJ39KuAYWY2xMyygDnA4gZlFgO3R/OzgKUeBqN9D7gWwMxygcnAxtYIvFG9L9QdvUhMTJo0qd674o8++ihjx45l8uTJ7Nixg3feeee0fYYMGcK4ceMAmDhxItu3b2/y+ysqKigvL2fq1KkA3H777axYsQKAMWPGcOutt/LTn/6UjIxwPzxlyhS++tWv8uijj1JeXn5qfWfQbKTuXmVmdwFLgHTgCXdfb2b3AiXuvhh4HHjazDYD+wmVAYS3dZ40s/WAAU+6+9q2OBEg3NFvfBFqqiEtvc0OIxJnZ7rzbk+5ubmn5pcvX87LL7/Mq6++Svfu3bn66qsbfZc8Ozv71Hx6enqzTTdNefHFF1mxYgUvvPAC3/nOd3jzzTeZN28eH//4x3nppZeYMmUKS5Ys4bLLLjun729vSVVJ7v4S8FKDdfckzFcSXqVsuN/hxta3mfwiqDkJh3ZDXmG7HVZEWqZnz55nbPOuqKggPz+f7t27s3HjRl577bUWHzMvL4/8/HxeeeUVrrzySp5++mmmTp1KTU0NO3bs4JprruHDH/4wCxYs4PDhw+zbt4/Ro0czevRoVq1axcaNG+OV6DuNU2/evKtEL9KJFBQUMGXKFEaNGsWMGTP4+Mc/Xm/79OnT+eEPf8jw4cO59NJLmTx5cqsc96mnnuLOO+/k6NGjXHTRRTz55JNUV1dz2223UVFRgbtz991307t3b/7pn/6JZcuWkZaWxsiRI5kxY0arxNAeLDSldxzFxcV+zgOP7NsC350AM78P429t3cBEYmzDhg0MHz481WFIkhq7Xma22t2LGysfr75u8goB05s3IiIJ4pXoM7Kh10C9eSMikiBeiR70Lr2ISAPxS/S9L1Q3CCIiCeKX6PMvDK9XntRIOSIiEMtEXxQ+K3acsZiISFcRv0Sf+C69iMRWjx49ANi1axezZs1qtMzVV19Nc69rP/zwwxw9evTU8tl2e9yUjtQdcvwSfX5tot+W2jhEpF0MGDCARYsWnfP+DRN9HLs9jl+i73EBpGfrzRuRTmTevHk89thjp5Zr74YPHz7MtGnTmDBhAqNHj+aXv/zlaftu376dUaNGAXDs2DHmzJnD8OHDuemmm+r1dfOlL32J4uJiRo4cyTe/+U0gdJS2a9currnmGq655hqgrttjgAcffJBRo0YxatQoHn744VPH62zdIcerCwSAtDToPVhNNyLn6tfz4P03W/c7LxgNM+5rcvPs2bP5yle+wpe//GUAFi5cyJIlS8jJyeG5556jV69e7N27l8mTJ3PDDTc0OW7qD37wA7p3786GDRtYu3YtEyZMOLXtO9/5Dn369KG6uppp06axdu1a7r77bh588EGWLVtG3759633X6tWrefLJJ1m5ciXuzhVXXMHUqVPJz8/vdN0hx++OHvQuvUgnM378ePbs2cOuXbt44403yM/PZ9CgQbg73/jGNxgzZgwf+chH2LlzJx988EGT37NixYpTCXfMmDGMGTPm1LaFCxcyYcIExo8fz/r163nrrbfOGNMf/vAHbrrpJnJzc+nRowef/OQneeWVV4DO1x1y/O7oITyQLV2V6ihEOqcz3Hm3pZtvvplFixbx/vvvM3v2bAB+9rOfUVZWxurVq8nMzKSoqKjR7ombs23bNh544AFWrVpFfn4+n/vc587pe2p1tu6Q43tHX1kBx8pTHYmIJGn27NksWLCARYsWcfPNoXfziooKzjvvPDIzM1m2bBnvvnvmv9Svuuoqfv7znwOwbt061q4Nw18cPHiQ3Nxc8vLy+OCDD/j1r399ap+muki+8soref755zl69ChHjhzhueee48orrzzr80rsDhlotDvk+++/n4qKCg4fPsyWLVsYPXo0X//617n88svZuLHlYzUldUdvZtOBRwgDj/zY3e9rsD0b+AkwkTCE4Gx3325mtwJfSyg6Bpjg7mtaHPmZ1L5LX/4udOvdpocSkdYxcuRIDh06xMCBA+nfvz8At956K9dffz2jR4+muLi42TvbL33pS3z+859n+PDhDB8+nIkTJwIwduxYxo8fz2WXXcagQYOYMmXKqX3mzp3L9OnTGTBgAMuWLTu1fsKECXzuc59j0qRJAHzhC19g/PjxZ2ymaUqqu0NutptiM0sH3gY+CpQShha8xd3fSijzX4Ex7n6nmc0BbnL32Q2+ZzTwvLtffKbjtaib4lq71sD8qfDpn8CImS37LpEuQN0Udy5t0U3xJGCzu2919xPAAqBh9pwJPBXNLwKm2emPxW+J9m17BUMBgz0b2uVwIiIdWTKJfiCQ2J9AabSu0TLuXgVUAAUNyswGnmnsAGY218xKzKykrKwsmbjPLLtHSPa72254WhGRzqJdHsaa2RXAUXdf19h2d5/v7sXuXtyvX7/WOWj/MfC+Er1IsjraaHPSuHO5Tskk+p3AoITlwmhdo2XMLAPIIzyUrTWHJu7m28wFY0LHZkf3t+thRTqjnJwc9u3bp2Tfwbk7+/btO+sfUSXz1s0qYJiZDSEk9DnAZxqUWQzcDrwKzAKWevQvxszSgE8DZ/9eUkv0j34o8f5auOjqdj20SGdTWFhIaWkprdJ0Km0qJyeHwsLCs9qn2UTv7lVmdhewhPB65RPuvt7M7gVK3H0x8DjwtJltBvYTKoNaVwE73H3rWUXWUheMDZ+7lehFmpOZmcmQIUNSHYa0kaTeo3f3l4CXGqy7J2G+Eri5iX2XA5PPPcRzlFsQxo/d/Ua7H1pEpCOJ5y9ja12gB7IiIvFO9P3Hwt534MSRVEciIpIyMU/0YwCHD9anOhIRkZSJd6K/IHrzRu30ItKFxTvR5xVCt3y104tIlxbvRG8W7urVFYKIdGHxTvQQ2un3vAXVJ1MdiYhISsQ/0V8wFqpPQNmmVEciIpIS8U/0/fVAVkS6tvgn+oKh4YHsu39MdSQiIikR/0Sflh76utmyFNQzn4h0QfFP9AAXXwuHdkNZywfZFRHpbLpGor/omvC5ZWlq4xARSYGukeh7D4K+lyjRi0iX1DUSPYTmm+1/hJOVqY5ERKRdda1EX3UMdryW6khERNpVUonezKab2SYz22xm8xrZnm1mz0bbV5pZUcK2MWb2qpmtN7M3zezsBjtsLRdOgbRM2Py7lBxeRCRVmk30ZpYOPAbMAEYAt5jZiAbF7gAOuPtQ4CHg/mjfDOCnwJ3uPhK4GkhNXwTZPWDwZNiyLCWHFxFJlWTu6CcBm919q7ufABYAMxuUmQk8Fc0vAqaZmQHXAWvd/Q0Ad9/n7tWtE/o5uPga+OBNOPRBykIQEWlvyST6gcCOhOXSaF2jZdy9CqgACoBLADezJWb2upn9fWMHMLO5ZlZiZiVtOgr9xdeGz626qxeRrqOtH8ZmAB8Gbo0+bzKzaQ0Luft8dy929+J+/fq1XTQXjIXc82DTS82XFRGJiWQS/U5gUMJyYbSu0TJRu3wesI9w97/C3fe6+1HgJWBCS4M+Z2lpMOIGePu3GkdWRLqMZBL9KmCYmQ0xsyxgDrC4QZnFwO3R/Cxgqbs7sAQYbWbdowpgKvBW64R+jkbeFF6zfHtJSsMQEWkvzSb6qM39LkLS3gAsdPf1Znavmd0QFXscKDCzzcBXgXnRvgeABwmVxRrgdXd/sdXP4mwM/hD0OB/WP5fSMERE2ktGMoXc/SVCs0viunsS5iuBm5vY96eEVyw7hrR0GDETXv8JHD8cXrsUEYmxrvPL2EQjboSqSnj7N6mORESkzXXNRD94MvS4QM03ItIldM1EX9t8885/wPFDqY5GRKRNdc1ED+Htm+rjsEnNNyISb1030Q+6AnoNhDcXpjoSEZE21XUTfVoajJkNm1+Gg7tTHY2ISJvpuokeYPxt4DXwxjOpjkREpM107URfcDEM/gtY8zNwT3U0IiJtomsneoDxt8K+zbBjZaojERFpE0r0I26EzFz4c8f58a6ISGtSos/uEV61XP+cerQUkVhSoofwUPbEYXjrl6mORESk1SnRQ+gSoWAolDyZ6khERFqdEj2AGVz+BSj9E+z6c6qjERFpVUr0tcZ9JjyUXTk/1ZGIiLQqJfpaOXkh2a9bBIfbcIByEZF2llSiN7PpZrbJzDab2bxGtmeb2bPR9pVmVhStLzKzY2a2Jpp+2Mrxt65Jc6H6BLz+r6mORESk1TSb6M0sHXgMmAGMAG4xsxENit0BHHD3ocBDwP0J27a4+7hourOV4m4b/S6Bi6+FVU9A9clURyMi0iqSuaOfBGx2963ufgJYAMxsUGYm8FQ0vwiYZmbWemG2o0l/DYd2wYYXUh2JiEirSCbRDwR2JCyXRusaLRMNJl4BFETbhpjZn83s92Z2ZWMHMLO5ZlZiZiVlZSluHx92HeQPgde+r/5vRCQW2vph7G5gsLuPB74K/NzMejUs5O7z3b3Y3Yv79evXxiE1Iy0N/uK/Qekq2LYitbGIiLSCZBL9TmBQwnJhtK7RMmaWAeQB+9z9uLvvA3D31cAW4JKWBt3mxt0KPfvDin9OdSQiIi2WTKJfBQwzsyFmlgXMARY3KLMYuD2anwUsdXc3s37Rw1zM7CJgGLC1dUJvQ5k58Bd3w/ZX4L3XUh2NiEiLNJvoozb3u4AlwAZgobuvN7N7zeyGqNjjQIGZbSY00dS+gnkVsNbM1hAe0t7p7vtb+RzaxsTboXtfWPFAqiMREWkR8w72wLG4uNhLSkpSHUbwyv+D390Lc5fDgPGpjkZEpElmttrdixvbpl/GnsnlXwy/mP292upFpPNSoj+TnF7wobtg04tqqxeRTkuJvjkf+nJ4A2fJP+q9ehHplJTom5OVC9f+D9hZEkahEhHpZJTokzH2Fjh/FLz8Lag6nupoRETOihJ9MtLS4br/CeXvwp/UX72IdC5K9Mm6+FoY+pHwBs6h91MdjYhI0pToz8b0+6GqEn5zWpf8IiIdlhL92eg7FK76u/BQ9u3fpjoaEZGkKNGfrSlfgb6Xwot/CyeOpDoaEZFmKdGfrYwsuP4RqHgPlv+fVEcjItIsJfpzceGHYOLn4NXH9ItZEenwlOjP1Uf/J+QNgl/MhcqDqY5GRKRJSvTnKqcXfPJHULEDfv31VEcjItIkJfqWGHwFXPU1eOPn6h5BRDqspBK9mU03s01mttnMTnuJ3MyyzezZaPtKMytqsH2wmR02s79rpbg7jqu+BgMnwgt/Awe2pzoaEZHTNJvoo6EAHwNmACOAW8xsRINidwAH3H0o8BBwf4PtDwK/bnm4HVB6Jnzq8TD/7G1w8lhq4xERaSCZO/pJwGZ33+ruJ4AFwMwGZWYCT0Xzi4BpZmYAZnYjsA1Y3yoRd0R9hoT2+vffhF99Vd0Zi0iHkkyiHwjsSFgujdY1WiYaY7aCMIZsD+DrwLfPdAAzm2tmJWZWUlZWlmzsHcslfwlT54X2+pLHUx2NiMgpbf0w9lvAQ+5++EyF3H2+uxe7e3G/fv3aOKQ2NPXrMOw6+PU82PZKqqMREQGSS/Q7gUEJy4XRukbLmFkGkAfsA64A/q+ZbQe+AnzDzO5qWcgdWFpaaMLpcxE8eyuUbUp1RCIiSSX6VcAwMxtiZlnAHGBxgzKLgduj+VnAUg+udPcidy8CHgb+t7t/r3VC76C69YZb/w3Ss+BnN8PhTtoUJSKx0Wyij9rc7wKWABuAhe6+3szuNbMbomKPE9rkNwNfBbp2P775F8Itz8LhPfDMbHV+JiIpZd7B3hApLi72kpKSVIfROjb8Chb+FRR9GD6zEDK7pToiEYkpM1vt7sWNbdMvY9vS8E/AjT8ID2af/SuNNysiKaFE39bGzoHrH4bN/wH/9nmoPpnqiESki1Gibw8TPwcz/hk2vQgLbtWvZ0WkXSnRt5cr5sInHoJ3fgs//ZS6NhaRdqNE356K/wt86sewYyU8db1evRSRdqFE395Gz4I5Pw8/pvrRtfD+ulRHJCIxp0SfCpf8JXz+Jag5CY9fBxtfTHVEIhJjSvSpMnACfHEZ9Ls0PKBdfh/UVKc6KhGJISX6VOrVP9zZj5kNy/9PeEirdnsRaWVK9KmW2Q1u+iFc/yi8+5/wL1fC9j+kOioRiREl+o7ADCbeDl94GTK7w79+An77P/RLWhFpFUr0HUn/MXDnK1D8efjP78L8a2D32lRHJSKdnBJ9R5OVG35Y9ZmFcKQM5l8NL39bv6YVkXOmRN9RXfKX8OWVoa+cPzwIP5gC21akOioR6YSU6Duy7n3gxu/DXz0PNVXh17QLPwvl76U6MhHpRJToO4OLrwl399f8I7z9W/je5bD0f8HxQ6mOTEQ6gaQSvZlNN7NNZrbZzE4bPcrMss3s2Wj7SjMritZPMrM10fSGmd3UyvF3HZndYOrfw38rgcs+ASv+GR4ZByvnQ9WJVEcnIh1Ys4nezNKBx4AZwAjgFjMb0aDYHcABdx8KPATcH61fBxS7+zhgOvAv0eDhcq7yCmHW4/DFpXDecPj11+B7xfDnn0J1VaqjE5EOKJk7+knAZnff6u4ngAXAzAZlZgJPRfOLgGlmZu5+NBpzFiAH6FjjFnZmAyfC7S/ArYugWz788ssh4a/5uQY3EZF6kkn0A4EdCcul0bpGy0SJvQIoADCzK8xsPfAmcGdC4j/FzOaaWYmZlZSVqQuApJnBsI/C3OUw5xnI7gnPfwm+OxFW/6uadEQEaIeHse6+0t1HApcD/2BmOY2Ume/uxe5e3K9fv7YOKX7M4LKPwV+vgFsWQPcCeOFv4NFxoQ3/ZGWqIxSRFEom0e8EBiUsF0brGi0TtcHnAfsSC7j7BuAwMOpcg5VmmMGlM0L7/W2/gLxBoQ3/kbHw6mNw4kiqIxSRFEgm0a8ChpnZEDPLAuYAixuUWQzcHs3PApa6u0f7ZACY2YXAZcD2VolcmmYGQ6fBf/lNaMfvOwyWfAMeHh3e1jlWnuoIRaQdNfsGjLtXmdldwBIgHXjC3deb2b1AibsvBh4HnjazzcB+QmUA8GFgnpmdBGqA/+rue9viRKQRZjDkqjC99xq88v/C+/d/eATGfQYmzYW+Q1MdpYi0MXPvWC/CFBcXe0lJSarDiK/da0OHaeufCyNcDf0ITPw8XDId0vXmq0hnZWar3b240W1K9F3UoQ/Cmzmrn4RDu6HHBTD+tnCnX3BxqqMTkbOkRC9Nq66Cd34bkv47vwUcBl0BY2+BkTeGd/RFpMNTopfkVOyENxfCmmdg7yZIywzv6Y/6VHibJys31RGKSBOU6OXsuMPuNfDmIlj3Czi0CzK6wbCPwIgbQxfK2T1THaWIJFCil3NXUwPv/Sesfx42LIbDH0B6Flx0dehc7ZLp0PP8VEcp0uUp0UvrqKmGHSthw69g4wt1/eIPGB8S/rCPQv/xkKber0XamxK9tD53+GA9vP0beHsJlK4CPHS/cPE0uPjacNffq3+qIxXpEs6U6PXitJwbM7hgVJiu+js4she2LIPNL4fpzYWhXL/LYMhUGHIlXDgljJolIu1Kd/TS+mpq4IN1sHVZSP7vvQZVx4Cochj8F3BhNPU4L9XRisSCmm4ktaqOw87VsO0VePePsONPUeIH8ovCe/uDJkHh5XDeSP1CV+QcqOlGUisju+4OHsLAKLvWwI7XwsPdrcth7bNhW2b38HB34AQYWAyFxdBrYGgqEpFzojt6ST338AZP6aq66f03oToaOKV7X+g/FgaMgwvGwAWjIX+I3u4RSaA7eunYzCD/wjCNnhXWVR2H99fBrtfD3f/uN+CPj0BNNEBZVk84f2Ro8z9/JJw/Kjz4zemVstMQ6aiU6KVjysiGwolhqnWyEso2hLv93WvDA9+1C+H4wboyeYPDoOnnXQb9hkO/S6HvJZDdo/3PQaSDUKKXziMzJ7TfDxhft84dyt+FD96CPbXTRtiyNHTDXKtXIfS7JCT9vsOgYFj47Nlf7f8Se0r00rmZhTd38ovCuLm1qk/C/q1Qtil00Fa2Cfa+Da//BE4erSuX1QP6XAQFQ6PpYuhzcVjXvY8qAYmFpBK9mU0HHiGMMPVjd7+vwfZs4CfARMJYsbPdfbuZfRS4D8gCTgBfc/elrRi/SOPSM0OzTb9L66+vqQmdtO19B/Ztrpt2vQ5vPQ9eU1c2Ow/6FIUHv7WVSe2UVxiOIdIJNJvozSwdeAz4KFAKrDKzxe7+VkKxO4AD7j7UzOYA9wOzgb3A9e6+y8xGEYYjHNjaJyGStLS0kKTzCuHia+pvqzoe3v7ZtyX8NbB/KxzYFp4JbPxV3YNgAEsLzUG9B4eHyHmDwnzvQeG7exVCRlb7nptIE5K5o58EbHb3rQBmtgCYCSQm+pnAt6L5RcD3zMzc/c8JZdYD3cws292PtzhykdaWkR3a7fsOO31bTTUc3AUHtoep/F048G743LIsjNJF4qvKFn7122sg5A0Mib/XgGgaGD579ldlIO0imUQ/ENiRsFwKXNFUmWgw8QqggHBHX+tTwOuNJXkzmwvMBRg8eHDSwYu0m7T0cLfee1Dot6ehqhNQsQMqSsNn+Q44uDNMZW+HyuDE4dP36943dPzWc0D02R96XlD32eMCyO0bji9yjtrlYayZjSQ051zX2HZ3nw/Mh/CDqfaISaRVZWSFB7lnGm+3siKM4nVoFxzcHf5CqJ0/tCs8JzhSdvp+lh7+OqhN/D3Ogx7nh8/cfnWfuf0gJ08PkOU0yST6ncCghOXCaF1jZUrNLAPIIzyUxcwKgeeAz7r7lhZHLNJZ5eSF6fwRTZepOhEGdzn8QWgOOvR+3XT4fThYWlchJD44rpWeHSX9vnXJP7dvmLonfhaEz6xcVQxdQDKJfhUwzMyGEBL6HOAzDcosBm4HXgVmAUvd3c2sN/AiMM/d/9hqUYvEVUZWXRPRmdRUw9F9cHgPHNkDh8uizz2hEjiyN3zueSvMVzfxWCwjp37iz+0bxhTo1ie8Xtq9oMHUR28bdULNJvqozf0uwhsz6cAT7r7ezO4FStx9MfA48LSZbQb2EyoDgLuAocA9ZnZPtO46d9/T2ici0qWkRc05yXTz7B6eDxwpgyP7QgVxdG9dZXA0WndkL+x7B44egBOHmv6+nLxQEXTLD4m/W37d8mnreof5nN7qmyiF1KmZiJyu6jgc3Q/H9ocK4OjesHx0f5g/dqBu+7EDYaqsOMMXWlRB5DeYetfN5/Suvy4nL6zL7KbmpSSoUzMROTsZ2eEtoLMZCrK6KiT7YwfqVwC1lUJlef35A9vqKojGnjfUSs+qe76R0zt0XJeTB9m9EtYnTNm9Qpnaz6yeXf6vCSV6EWkd6RmhrT+34Oz2q6kJHdMd2w/HyqMKIfqsrIjmK6KpHCoPhtdXKyvCflWVzRzAILtnQgXQcL5n6AojKzeMh1C7rt7UK5TJyO6Uf10o0YtIaqWlRc01vc9t/5OVdUm/8iAcr4g+DzbyWQHHD4XmpwPbwnzlwboRz5qNNTP0hJrVM3zWVhLZPeuvz+pRt5yVW7cuK7f+Zzv9YE6JXkQ6t8ycMPU8/9y/o7oqdHZ34kh4cH38UN10avkgHD98+vrK8vBDucT1JPnsMy2zfvK/dDp89N5zP48mKNGLiKRnQHqv1hm4pqYm/IVw/HBI+icOR/NHwttMxw9HlcrhhPmogunet+XHb4QSvYhIa0pLi+7Sc4EW/JXRirr2o2gRkS5AiV5EJOaU6EVEYk6JXkQk5pToRURiToleRCTmlOhFRGJOiV5EJOaU6EVEYi6pRG9m081sk5ltNrN5jWzPNrNno+0rzawoWl9gZsvM7LCZfa+VYxcRkSQ0m+jNLB14DJgBjABuMbOGg17eARxw96HAQ4SBwAEqgX8C/q7VIhYRkbOSzB39JGCzu2919xPAAmBmgzIzgaei+UXANDMzdz/i7n8gJHwREUmBZBL9QGBHwnJptK7RMu5eBVQASY8+YGZzzazEzErKysqS3U1ERJLQIR7Guvt8dy929+J+/fqlOhwRkVhJJtHvBAYlLBdG6xotY2YZQB6wrzUCFBGRlkkm0a8ChpnZEDPLAuYAixuUWQzcHs3PApa6e5JDrIiISFtqduARd68ys7uAJUA68IS7rzeze4ESd18MPA48bWabgf2EygAAM9sO9AKyzOxG4Dp3f6vVz0RERBqV1AhT7v4S8FKDdfckzFcCNzexb1EL4hMRkRbqEA9jRUSk7SjRi4jEnBK9iEjMKdGLiMScEr2ISMwp0YuIxJwSvYhIzCnRi4jEnBK9iEjMKdGLiMScEr2ISMwp0YuIxFxsEr27s7a0HPWOLCJSX1K9V3YGf95Rzie//58M7N2NT4zpz8dG92f0wDzS0izVoYmIpFRsEv2w83rwwM1jeXHtLh7/wzb+ZcVWeuVkMOHCfCYOzmdUYR4j+vfivJ7ZmCn5i0jXEZtE3zMnk1kTC5k1sZDyoydYunEPq7YfYPW7+1m+qW7A8YLcLIr65jIovxuF+d0Z0Lsb/XvnMCCvG+f1zCavW6b+ChCRWLFk2rTNbDrwCGGEqR+7+30NtmcDPwEmEsaKne3u26Nt/wDcAVQDd7v7kjMdq7i42EtKSs7+TM6g4thJNu4+yIbdB9mw+xDv7j9C6YFj7K6opLqm/vmnpxn53bPI65ZBbnYG3TLTyclMJyPNSE8zMtPTyM5IIzszjeyMdLIy0siK1mVl1H6mk5FuZKaH8pnpYVtmWhppaZBmhgFpaUaagZmRFZXLTA/HSbPwmZFmZKSnkZEe5tPMTsWiv0xEpJaZrXb34sa2NXtHb2bpwGPAR4FSYJWZLW4wHOAdwAF3H2pmc4D7gdlmNoIwrOBIYADwspld4u7VLTuls5PXLZMrLirgiosK6q2vqq5hz6Hj7K44xq7ySsoOHWf/kRPsO3Kcg5VVHD1exdET1ZQfO0lNjVNV45yoquZEdQ2VJ2s4fjLMH6+qIRXPgEMlECqTrIQKJTsjjZzMdDLT6yqDhhV6mtmpiiYjLY20NCPdqFfJpJlhFpW12orJSDcjPT36PFW+/vbwfWG9ReXSo+9LT6io0hK/3+qOHSrVhFgSKrm6MtSLtXa/2sqz3vemhcq19nwMIGHeaitfM7D65cKqEDs03JZQRhWvdFDJNN1MAja7+1YAM1sAzAQSE/1M4FvR/CLgexb+1c8EFrj7cWBbNKbsJODV1gm/ZTLS0xjQuxsDendj4oXn/j3utZVADSeqQuI/WV1DVY1zsjqsO1ldw8lqp8bD5A7uUONOtTtV1XVlq2vCutrKpSr6rqoaD9sarD9RVUNVTQ0nq5zjVdUcr6qh8mQ1J6vrJ/faPFR73Bp3amrgWHU1VTXheDUevj9sj8rV1J+vdqe6BqpraqKynIrZT+3fggvTyUV1Rf1KIKoo6s0nlCFxuZH9o2+u9/0kfPfp6+sqncT6p948p5epPW79Iza20MS5n/bfonUrv6ZaIFL6z62JgzcV05laUa657Dy+ef3IlsfUQDKJfiCwI2G5FLiiqTLRYOIVQEG0/rUG+w5seAAzmwvMBRg8eHCysXcYZnXNNLnZqY6mY3CvqwBOVSq1y7UVijs4Yb3Xr2iqG1Rstd9RXQNVNTXU1NTtk1gxVkeVaGKFWrt/dLh6836q0vVoW93/iPXLgeOn/nKrPb/E9R42nFb+tOVmvrfR9af+u9Yt1d4shDX1yyaur7sojc6eOt/E/Zoqc8Zr3uyK5DleryKq5+xWt4umKrSmYmqq/ruwT/fWCaiBDvEw1t3nA/MhtNGnOBxpBWZ1TUEiklrJ/GBqJzAoYbkwWtdoGTPLAPIID2WT2VdERNpQMol+FTDMzIaYWRbh4eriBmUWA7dH87OApR7+1lsMzDGzbDMbAgwD/tQ6oYuISDKabbqJ2tzvApYQXq98wt3Xm9m9QIm7LwYeB56OHrbuJ1QGROUWEh7cVgFfbu83bkREurqk3qNvT23xHr2ISNyd6T362HRqJiIijVOiFxGJOSV6EZGYU6IXEYm5Dvcw1szKgHdb8BV9gb2tFE5n0RXPGbrmeeucu46zPe8L3b1fYxs6XKJvKTMraerJc1x1xXOGrnneOueuozXPW003IiIxp0QvIhJzcUz081MdQAp0xXOGrnneOueuo9XOO3Zt9CIiUl8c7+hFRCSBEr2ISMzFJtGb2XQz22Rmm81sXqrjaQtmNsjMlpnZW2a23sz+Jlrfx8z+w8zeiT7zUx1rWzCzdDP7s5n9KloeYmYro2v+bNSNdmyYWW8zW2RmG81sg5l9qCtcazP779G/73Vm9oyZ5cTxWpvZE2a2x8zWJaxr9Ppa8Gh0/mvNbMLZHCsWiT5hAPMZwAjglmhg8ripAv7W3UcAk4EvR+c5D/iduw8Dfhctx9HfABsSlu8HHnL3ocABwiD1cfII8Bt3vwwYSzj3WF9rMxsI3A0Uu/soQtfoc4jntf5XYHqDdU1d3xmE8TyGEYZd/cHZHCgWiZ6EAczd/QRQO4B5rLj7bnd/PZo/RPgffyDhXJ+Kij0F3JiSANuQmRUCHwd+HC0bcC1hMHqI2XmbWR5wFWGsB9z9hLuX0wWuNWGcjG7RaHXdgd3E8Fq7+wrC+B2Jmrq+M4GfePAa0NvM+id7rLgk+sYGMD9tEPI4MbMiYDywEjjf3XdHm94Hzk9VXG3oYeDvgZpouQAod/eqaDlu13wIUAY8GTVX/djMcon5tXb3ncADwHuEBF8BrCbe1zpRU9e3RTkuLom+SzGzHsC/A19x94OJ26IhHGP1zqyZfQLY4+6rUx1LO8oAJgA/cPfxwBEaNNPE9FrnE+5ehwADgFxOb97oElrz+sYl0XeZQcjNLJOQ5H/m7r+IVn9Q+2dc9LknVfG1kSnADWa2ndAsdy2h/bp39Oc9xO+alwKl7r4yWl5ESPxxv9YfAba5e5m7nwR+Qbj+cb7WiZq6vi3KcXFJ9MkMYN7pRe3SjwMb3P3BhE2Jg7PfDvyyvWNrS+7+D+5e6O5FhGu71N1vBZYRBqOHmJ23u78P7DCzS6NV0whjL8f6WhOabCabWffo33vtecf2WjfQ1PVdDHw2evtmMlCR0MTTPHePxQR8DHgb2AL8Y6rjaaNz/DDhT7m1wJpo+hihvfp3wDvAy0CfVMfahv8NrgZ+Fc1fBPwJ2Az8G5Cd6vha+VzHASXR9X4eyO8K1xr4NrARWAc8DWTH8VoDzxCeQ5wk/AV3R1PXFzDCm4VbgDcJbyUlfSx1gSAiEnNxaboREZEmKNGLiMScEr2ISMwp0YuIxJwSvYhIzCnRi4jEnBK9iEjM/X9TPIOQIFPH1wAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the loss for all the models\n",
        "plt.plot(relu_model.train_loss, label='relu')\n",
        "plt.plot(sigmoid_model.train_loss, label='sigmoid')\n",
        "plt.plot(tanh_model.train_loss, label='tanh')\n",
        "plt.plot(leaky_relu_model.train_loss, label='leaky_relu')\n",
        "plt.plot(linear_model.train_loss, label='linear')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the loss for all the models\n",
        "plt.plot(relu_model.val_loss, label='relu')\n",
        "plt.plot(sigmoid_model.val_loss, label='sigmoid')\n",
        "plt.plot(tanh_model.val_loss, label='tanh')\n",
        "plt.plot(leaky_relu_model.val_loss, label='leaky_relu')\n",
        "plt.plot(linear_model.val_loss, label='linear')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "a5a87ee616be0254e3f1af9223138e3faeac65b2c9d91bc22a9fc5a4a8bd8eb0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
